\chapter{Conclusion}
\label{chapter:conclusion}

This chapter will summarize the paper by giving an overall impression of the two optimization techniques implemented, comparing the project's end products with the initial goals, discussing any difficulties encountered, and describing plans for further work.

\section{Summary of Results}

The results indicate a clear winner in terms of the optimization techniques; the temporal cache. It gave performance benefits in almost all situations it was tested in, even during movement, which was its biggest weakness. The SDF, on the other hand, performed poorly in all situations it was tested in. I think this was because the signed distance functions used were simply not computationally expensive enough to justify the use of an SDF. Additionally, the range of the SDF was so limited in the case of the Hall of Pillars fractal, it wasn't worth using at all. I do think, however, that there are some things I could do to try to make the method more viable. These will be discussed in the section \ref{section:further-work}, below.

\section{Project Completion}

The table below summarizes the main goals laid out at the start of the project, along with their completion level.\newline

\begin{tabular}{||p{0.25\linewidth}|p{0.2\linewidth}|p{0.45\linewidth}||}
	\hline
	Goal & Completion Level & Evidence\\
	\hline\hline
	Project & Complete & Project\\
	\hline
\end{tabular}

\section{Difficulties}

There were a couple of significant difficulties that arose. First, the Vulkan setup took much longer than expected, leaving less time for implementation of the optimization methods. It would have been nice to have more time for experimentation (I had lots of ideas in mind), and time to try to improve the results with the SDF, but overall this hiccup did not derail the project.\newline

The second difficulty was, of course, the performance of the SDF. While this is not necessarily a terrible thing, since I did implement a method that worked in the end, I did spend quite a bit of time trying to improve the SDF and taking performance measurements with different configurations, to no avail. This also left me with less time to try to improve the temporal cache method, which would have been more fruitful, I think.

\section{Further Work}\label{section:further-work}

This section will be split into subsections, each tackling a different area of the project I would like to work on in future.

\subsection{Rendering}

One drawback of the temporal cache is that it ruins the free ambient occlusion effect that comes with the sphere tracing algorithm. Implementing some kind of method to reduce this would be an interesting task. Perhaps keeping track of the number of iterations during the last complete raycast would work.\newline

Lighting is another thing that could be implemented. It's possible to calculate normals based on the gradient of the signed distance function along the ray. I should think that using the temporal cache would ruin this as well, but there may be another acceptable way to calculate normals.

\subsection{Project Application: Game}

Eventually, I would like to use fractals as terrain in a game. This will require me to implement a collision system for the fractals; this could be done simply by evaluating the signed distance function for the fractal, and declaring that a collision has occurred if the player is within some distance. Being able to influence the terrain would be excellent as well. During the Hall of Pillars flythrough, the camera moves through the ceiling, which could be considered immersion-breaking. Instead, it would be better to be able to, for example, drill tunnels through the fractal. I would implement this by storing a set of primitives that represent these holes, and if the ray is travelling through one of them, the fractal will not be calculated. I'm particularly excited about the possibility of morphing the terrain during the game, and implementing physics-based puzzles using this mechanic.

\subsection{SDF}

First and foremost, making the SDF more memory efficient would be very important. During development, the first SDF implemented did actually attempt (successfully, I might add; the number of voxels was reduced by a factor of 10) to construct a sparse octree, based on the distance calculated at the centre of the cube. If the fractal surface was not likely to be inside the cube, the cube was not subdivided. However, the structure I came up with for traversing the octree was extremely inefficient, to the point that it made the SDF less memory efficient than it turned out to be in the end, with the regular octree. I would be interested in rolling back to this attempt and implementing it properly. This may allow the SDF to extend far beyond its current range.\newline

Additionally, finding a more computationally expensive fractal to use would be interesting, to see if the SDF actually has promise in some circumstances. It would be excellent, as the SDF is theoretically immune to the pitfalls of the temporal cache (movement).\newline

Lastly, finding a more efficient way to construct the SDF would be preferable. At the moment, it's generated offline on the CPU, but if I could move this work to a compute shader (which would be easier to parallelize with a dense octree than with a sparse one), it may be a lot quicker. I also had a plan to move the SDF with the camera, so that the camera is always surrounded by it, up to a certain view distance. I was going to achieve this by having a grid of SDF cubes. If the camera moved out of range of the end row or column, then the out of range one would be discarded and a new one would be generated in the direction of movement. If the SDF cubes were small enough, this could potentially be done in real time in the shaders.\newline

If an efficient method of generating the SDF in real time can be implemented, then the SDF would be able to handle changes in geometry, which it cannot currently do.

\subsection{Temporal Cache}

The most important piece of work to do here is to find a way to more accurately determine if a closer object has occluded the current ray. Improving this would result in less ray recasts, a reduced need for such a large underestimate of the true distance, less artefacts and better performance during movement.\newline

As mentioned in chapter \ref{chapter:implementation}, the image output from the geometry render pass is currently copied to the texture image to be sampled in the next frame, after the render pass. This could be avoided completely, by simply swapping the G-Buffer image and texture image after each frame. Another way this could be avoided is hopefully coming soon. There is to be a new Vulkan extension, `VK\_EXT\_attachment\_feedback\_loop\_layout', that will allow a single image to be both sampled from and used as a framebuffer image, in the same render pass. This sounds perfect for my purposes, as each pixel only writes to, and reads from, the same pixel each frame, so there would be no interference or data dependencies between pixels.